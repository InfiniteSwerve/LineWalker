{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thetis/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/thetis/.local/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from model import GPT\n",
    "from data_fetcher import (\n",
    "    get_padded_image,\n",
    "    get_line,\n",
    "    extract_patch,\n",
    "    get_line_data,\n",
    "    extract_patches,\n",
    "    generate_training_data,\n",
    "    get_answer_triplet,\n",
    "    compare_model_answer,\n",
    ")\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "from showmethetypes import SMTT\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = SMTT(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 10\n",
    "div = 16\n",
    "input_dim = window**2 * 4 * 3\n",
    "model = GPT(\n",
    "    num_layers=3,\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    d_hidden=2048,\n",
    "    input_dim=input_dim,\n",
    ").to(device=\"cuda\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "def loss_fn(prediction, target):\n",
    "    # row-wise MSE loss\n",
    "    squared_diff = (prediction - target) ** 2\n",
    "    row_wise_sum = torch.sum(squared_diff, dim=1)\n",
    "    row_wise_mse = torch.mean(row_wise_sum)\n",
    "    return row_wise_mse\n",
    "\n",
    "\n",
    "def row_wise_loss_fn(prediction, target):\n",
    "    squared_diff = (prediction - target) ** 2\n",
    "    row_wise_sum = torch.sum(squared_diff, dim=1)\n",
    "    return row_wise_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.10/site-packages/PIL/Image.py:3167: DecompressionBombWarning: Image size (122880000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "image = get_padded_image(\"./juneau2k.jpg\", window).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = get_line_data(range(9), split=False)\n",
    "train, test = d[], d[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list (10571)\n",
      "    |__tuple\n",
      "        |__Tensor (dtype: torch.float64)\n",
      "        |   |  (device: cpu)\n",
      "        |   |__dim_0 (2)\n",
      "        |   |__dim_1 (2)\n",
      "        |__Tensor (dtype: torch.float64)\n",
      "            |  (device: cpu)\n",
      "            |__dim_0 (2)\n"
     ]
    }
   ],
   "source": [
    "tt(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuple\n",
      "    |__Tensor (dtype: torch.float64)\n",
      "    |   |  (device: cpu)\n",
      "    |   |__dim_0 (2)\n",
      "    |   |__dim_1 (2)\n",
      "    |__Tensor (dtype: torch.float64)\n",
      "        |  (device: cpu)\n",
      "        |__dim_0 (2)\n"
     ]
    }
   ],
   "source": [
    "tt(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [2] at entry 0 and [] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_xs, train_ys \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(train))\n\u001b[0;32m----> 2\u001b[0m train_xs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack(train_xs)\n\u001b[1;32m      3\u001b[0m train_ys \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(train_ys)\n\u001b[1;32m      4\u001b[0m test_xs, test_ys \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(test))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2] at entry 0 and [] at entry 1"
     ]
    }
   ],
   "source": [
    "train_xs, train_ys = zip(*list(train))\n",
    "train_xs = torch.stack(train_xs)\n",
    "train_ys = torch.stack(train_ys)\n",
    "test_xs, test_ys = zip(*list(test))\n",
    "test_xs = torch.stack(test_xs)\n",
    "test_ys = torch.stack(test_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = generate_training_data(image, train_xs, window)\n",
    "testing_data = generate_training_data(image, test_xs, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor (dtype: torch.float32)\n",
      "    |  (device: cuda:0)\n",
      "    |__dim_0 (5768)\n",
      "    |__dim_1 (2)\n",
      "    |__dim_2 (1200)\n"
     ]
    }
   ],
   "source": [
    "tt(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Letting the model take the reins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Test Loss: 18.88450813293457 | Train Loss: 18.555727005004883\n",
      "Epoch: 1 | Test Loss: 16.961933135986328 | Train Loss: 16.640615463256836\n",
      "Epoch: 2 | Test Loss: 15.18541431427002 | Train Loss: 14.886537551879883\n",
      "Epoch: 3 | Test Loss: 13.583577156066895 | Train Loss: 13.287089347839355\n",
      "Epoch: 4 | Test Loss: 12.134140014648438 | Train Loss: 11.867840766906738\n",
      "Epoch: 5 | Test Loss: 10.847896575927734 | Train Loss: 10.592414855957031\n",
      "Epoch: 6 | Test Loss: 9.729561805725098 | Train Loss: 9.46135139465332\n",
      "Epoch: 7 | Test Loss: 8.731679916381836 | Train Loss: 8.498357772827148\n",
      "Epoch: 8 | Test Loss: 7.916683673858643 | Train Loss: 7.6756391525268555\n",
      "Epoch: 9 | Test Loss: 7.204083442687988 | Train Loss: 6.984828948974609\n",
      "Epoch: 10 | Test Loss: 6.629162311553955 | Train Loss: 6.419528007507324\n",
      "Epoch: 11 | Test Loss: 6.183938503265381 | Train Loss: 5.952271938323975\n",
      "Epoch: 12 | Test Loss: 5.822713375091553 | Train Loss: 5.608838081359863\n",
      "Epoch: 13 | Test Loss: 5.5555009841918945 | Train Loss: 5.373240947723389\n",
      "Epoch: 14 | Test Loss: 5.378735542297363 | Train Loss: 5.173109531402588\n",
      "Epoch: 15 | Test Loss: 5.256048679351807 | Train Loss: 5.060184955596924\n",
      "Epoch: 16 | Test Loss: 5.192430019378662 | Train Loss: 4.993622779846191\n",
      "Epoch: 17 | Test Loss: 5.168643474578857 | Train Loss: 4.9816389083862305\n",
      "Epoch: 18 | Test Loss: 5.173705101013184 | Train Loss: 4.980763912200928\n",
      "Epoch: 19 | Test Loss: 5.197311878204346 | Train Loss: 5.018957614898682\n",
      "Epoch: 20 | Test Loss: 5.229060649871826 | Train Loss: 5.0456719398498535\n",
      "Epoch: 21 | Test Loss: 5.272279739379883 | Train Loss: 5.0981950759887695\n",
      "Epoch: 22 | Test Loss: 5.319673538208008 | Train Loss: 5.14052677154541\n",
      "Epoch: 23 | Test Loss: 5.354368686676025 | Train Loss: 5.160893440246582\n",
      "Epoch: 24 | Test Loss: 5.379448890686035 | Train Loss: 5.215948581695557\n",
      "Epoch: 25 | Test Loss: 5.393557071685791 | Train Loss: 5.213542938232422\n",
      "Epoch: 26 | Test Loss: 5.397808074951172 | Train Loss: 5.217415809631348\n",
      "Epoch: 27 | Test Loss: 5.389176368713379 | Train Loss: 5.208250999450684\n",
      "Epoch: 28 | Test Loss: 5.373775959014893 | Train Loss: 5.193360328674316\n",
      "Epoch: 29 | Test Loss: 5.346115589141846 | Train Loss: 5.185258388519287\n",
      "Epoch: 30 | Test Loss: 5.3165435791015625 | Train Loss: 5.134069442749023\n",
      "Epoch: 31 | Test Loss: 5.27607536315918 | Train Loss: 5.105148792266846\n",
      "Epoch: 32 | Test Loss: 5.241813659667969 | Train Loss: 5.060608863830566\n",
      "Epoch: 33 | Test Loss: 5.200379848480225 | Train Loss: 5.026072025299072\n",
      "Epoch: 34 | Test Loss: 5.149044036865234 | Train Loss: 5.00197172164917\n",
      "Epoch: 35 | Test Loss: 5.108734607696533 | Train Loss: 4.941421985626221\n",
      "Epoch: 36 | Test Loss: 5.07991361618042 | Train Loss: 4.893392086029053\n",
      "Epoch: 37 | Test Loss: 5.034621238708496 | Train Loss: 4.865418910980225\n",
      "Epoch: 38 | Test Loss: 4.9967732429504395 | Train Loss: 4.832940101623535\n",
      "Epoch: 39 | Test Loss: 4.968101501464844 | Train Loss: 4.800545692443848\n",
      "Epoch: 40 | Test Loss: 4.945485591888428 | Train Loss: 4.782685279846191\n",
      "Epoch: 41 | Test Loss: 4.915342807769775 | Train Loss: 4.751512050628662\n",
      "Epoch: 42 | Test Loss: 4.90003776550293 | Train Loss: 4.739025115966797\n",
      "Epoch: 43 | Test Loss: 4.88099479675293 | Train Loss: 4.6954026222229\n",
      "Epoch: 44 | Test Loss: 4.874698638916016 | Train Loss: 4.705276012420654\n",
      "Epoch: 45 | Test Loss: 4.8561882972717285 | Train Loss: 4.687837600708008\n",
      "Epoch: 46 | Test Loss: 4.8416643142700195 | Train Loss: 4.671260833740234\n",
      "Epoch: 47 | Test Loss: 4.827289581298828 | Train Loss: 4.664610862731934\n",
      "Epoch: 48 | Test Loss: 4.828298568725586 | Train Loss: 4.653205871582031\n",
      "Epoch: 49 | Test Loss: 4.825622081756592 | Train Loss: 4.6622233390808105\n",
      "Epoch: 50 | Test Loss: 4.80855131149292 | Train Loss: 4.6298298835754395\n",
      "Epoch: 51 | Test Loss: 4.7950215339660645 | Train Loss: 4.625907897949219\n",
      "Epoch: 52 | Test Loss: 4.776835918426514 | Train Loss: 4.599068641662598\n",
      "Epoch: 53 | Test Loss: 4.775479316711426 | Train Loss: 4.6032209396362305\n",
      "Epoch: 54 | Test Loss: 4.758548259735107 | Train Loss: 4.584683895111084\n",
      "Epoch: 55 | Test Loss: 4.74207878112793 | Train Loss: 4.571427822113037\n",
      "Epoch: 56 | Test Loss: 4.724457740783691 | Train Loss: 4.562927722930908\n",
      "Epoch: 57 | Test Loss: 4.7072672843933105 | Train Loss: 4.543755054473877\n",
      "Epoch: 58 | Test Loss: 4.687539577484131 | Train Loss: 4.538240432739258\n",
      "Epoch: 59 | Test Loss: 4.6748199462890625 | Train Loss: 4.514058589935303\n",
      "Epoch: 60 | Test Loss: 4.655751705169678 | Train Loss: 4.49539852142334\n",
      "Epoch: 61 | Test Loss: 4.633537292480469 | Train Loss: 4.48862886428833\n",
      "Epoch: 62 | Test Loss: 4.622968673706055 | Train Loss: 4.467924118041992\n",
      "Epoch: 63 | Test Loss: 4.594295501708984 | Train Loss: 4.441924095153809\n",
      "Epoch: 64 | Test Loss: 4.575676918029785 | Train Loss: 4.423800945281982\n",
      "Epoch: 65 | Test Loss: 4.565693378448486 | Train Loss: 4.42018461227417\n",
      "Epoch: 66 | Test Loss: 4.545779228210449 | Train Loss: 4.3853840827941895\n",
      "Epoch: 67 | Test Loss: 4.538782119750977 | Train Loss: 4.385738372802734\n",
      "Epoch: 68 | Test Loss: 4.5114593505859375 | Train Loss: 4.356585502624512\n",
      "Epoch: 69 | Test Loss: 4.494978427886963 | Train Loss: 4.333621025085449\n",
      "Epoch: 70 | Test Loss: 4.47784948348999 | Train Loss: 4.328579902648926\n",
      "Epoch: 71 | Test Loss: 4.452505588531494 | Train Loss: 4.3115763664245605\n",
      "Epoch: 72 | Test Loss: 4.431737422943115 | Train Loss: 4.280513763427734\n",
      "Epoch: 73 | Test Loss: 4.416673183441162 | Train Loss: 4.262387752532959\n",
      "Epoch: 74 | Test Loss: 4.401449203491211 | Train Loss: 4.254767894744873\n",
      "Epoch: 75 | Test Loss: 4.370114803314209 | Train Loss: 4.225979328155518\n",
      "Epoch: 76 | Test Loss: 4.356273174285889 | Train Loss: 4.218122482299805\n",
      "Epoch: 77 | Test Loss: 4.335930347442627 | Train Loss: 4.188009738922119\n",
      "Epoch: 78 | Test Loss: 4.3139729499816895 | Train Loss: 4.182057857513428\n",
      "Epoch: 79 | Test Loss: 4.295793056488037 | Train Loss: 4.149773597717285\n",
      "Epoch: 80 | Test Loss: 4.267297267913818 | Train Loss: 4.127804756164551\n",
      "Epoch: 81 | Test Loss: 4.247511863708496 | Train Loss: 4.106438159942627\n",
      "Epoch: 82 | Test Loss: 4.226609230041504 | Train Loss: 4.097463607788086\n",
      "Epoch: 83 | Test Loss: 4.200736999511719 | Train Loss: 4.069393157958984\n",
      "Epoch: 84 | Test Loss: 4.175604820251465 | Train Loss: 4.042436599731445\n",
      "Epoch: 85 | Test Loss: 4.151176452636719 | Train Loss: 4.040050029754639\n",
      "Epoch: 86 | Test Loss: 4.137345790863037 | Train Loss: 3.9992566108703613\n",
      "Epoch: 87 | Test Loss: 4.109934329986572 | Train Loss: 3.986578941345215\n",
      "Epoch: 88 | Test Loss: 4.085636615753174 | Train Loss: 3.958258628845215\n",
      "Epoch: 89 | Test Loss: 4.052959442138672 | Train Loss: 3.938908100128174\n",
      "Epoch: 90 | Test Loss: 4.037528991699219 | Train Loss: 3.907803773880005\n",
      "Epoch: 91 | Test Loss: 4.009002685546875 | Train Loss: 3.878828525543213\n",
      "Epoch: 92 | Test Loss: 3.982375383377075 | Train Loss: 3.8736016750335693\n",
      "Epoch: 93 | Test Loss: 3.9528651237487793 | Train Loss: 3.850553512573242\n",
      "Epoch: 94 | Test Loss: 3.934561014175415 | Train Loss: 3.8123152256011963\n",
      "Epoch: 95 | Test Loss: 3.8971149921417236 | Train Loss: 3.789435386657715\n",
      "Epoch: 96 | Test Loss: 3.8694515228271484 | Train Loss: 3.769585132598877\n",
      "Epoch: 97 | Test Loss: 3.8482117652893066 | Train Loss: 3.7479023933410645\n",
      "Epoch: 98 | Test Loss: 3.8236398696899414 | Train Loss: 3.708177328109741\n",
      "Epoch: 99 | Test Loss: 3.7951478958129883 | Train Loss: 3.691180944442749\n",
      "Epoch: 100 | Test Loss: 3.761995553970337 | Train Loss: 3.6641547679901123\n",
      "Epoch: 101 | Test Loss: 3.736943244934082 | Train Loss: 3.6380090713500977\n",
      "Epoch: 102 | Test Loss: 3.7108259201049805 | Train Loss: 3.6028757095336914\n",
      "Epoch: 103 | Test Loss: 3.684443473815918 | Train Loss: 3.5857741832733154\n",
      "Epoch: 104 | Test Loss: 3.6489994525909424 | Train Loss: 3.5660665035247803\n",
      "Epoch: 105 | Test Loss: 3.6202619075775146 | Train Loss: 3.532747507095337\n",
      "Epoch: 106 | Test Loss: 3.5959386825561523 | Train Loss: 3.5034351348876953\n",
      "Epoch: 107 | Test Loss: 3.570341110229492 | Train Loss: 3.4782371520996094\n",
      "Epoch: 108 | Test Loss: 3.543156862258911 | Train Loss: 3.4665369987487793\n",
      "Epoch: 109 | Test Loss: 3.511911630630493 | Train Loss: 3.433408260345459\n",
      "Epoch: 110 | Test Loss: 3.487969398498535 | Train Loss: 3.420039176940918\n",
      "Epoch: 111 | Test Loss: 3.463050603866577 | Train Loss: 3.392751693725586\n",
      "Epoch: 112 | Test Loss: 3.433011531829834 | Train Loss: 3.3556313514709473\n",
      "Epoch: 113 | Test Loss: 3.409437894821167 | Train Loss: 3.356954336166382\n",
      "Epoch: 114 | Test Loss: 3.389923572540283 | Train Loss: 3.310776472091675\n",
      "Epoch: 115 | Test Loss: 3.358829975128174 | Train Loss: 3.3066678047180176\n",
      "Epoch: 116 | Test Loss: 3.3368306159973145 | Train Loss: 3.2755377292633057\n",
      "Epoch: 117 | Test Loss: 3.311081886291504 | Train Loss: 3.26941180229187\n",
      "Epoch: 118 | Test Loss: 3.3013153076171875 | Train Loss: 3.23555588722229\n",
      "Epoch: 119 | Test Loss: 3.2729458808898926 | Train Loss: 3.218987464904785\n",
      "Epoch: 120 | Test Loss: 3.24638032913208 | Train Loss: 3.200493812561035\n",
      "Epoch: 121 | Test Loss: 3.2291293144226074 | Train Loss: 3.1902272701263428\n",
      "Epoch: 122 | Test Loss: 3.206732749938965 | Train Loss: 3.1815459728240967\n",
      "Epoch: 123 | Test Loss: 3.18163800239563 | Train Loss: 3.1323344707489014\n",
      "Epoch: 124 | Test Loss: 3.172189950942993 | Train Loss: 3.150056838989258\n",
      "Epoch: 125 | Test Loss: 3.15494704246521 | Train Loss: 3.116987705230713\n",
      "Epoch: 126 | Test Loss: 3.1395885944366455 | Train Loss: 3.1047401428222656\n",
      "Epoch: 127 | Test Loss: 3.116983413696289 | Train Loss: 3.0991861820220947\n",
      "Epoch: 128 | Test Loss: 3.1028199195861816 | Train Loss: 3.081496000289917\n",
      "Epoch: 129 | Test Loss: 3.091916799545288 | Train Loss: 3.062168836593628\n",
      "Epoch: 130 | Test Loss: 3.0785133838653564 | Train Loss: 3.0456771850585938\n",
      "Epoch: 131 | Test Loss: 3.062079906463623 | Train Loss: 3.048921585083008\n",
      "Epoch: 132 | Test Loss: 3.0566678047180176 | Train Loss: 3.0364978313446045\n",
      "Epoch: 133 | Test Loss: 3.0408568382263184 | Train Loss: 3.0206644535064697\n",
      "Epoch: 134 | Test Loss: 3.032752275466919 | Train Loss: 3.0214452743530273\n",
      "Epoch: 135 | Test Loss: 3.0140538215637207 | Train Loss: 3.02630877494812\n",
      "Epoch: 136 | Test Loss: 3.0059332847595215 | Train Loss: 3.003272533416748\n",
      "Epoch: 137 | Test Loss: 2.990931272506714 | Train Loss: 2.9902150630950928\n",
      "Epoch: 138 | Test Loss: 2.9830968379974365 | Train Loss: 2.963137149810791\n",
      "Epoch: 139 | Test Loss: 2.9692318439483643 | Train Loss: 2.979175329208374\n",
      "Epoch: 140 | Test Loss: 2.9543282985687256 | Train Loss: 2.955991744995117\n",
      "Epoch: 141 | Test Loss: 2.9537816047668457 | Train Loss: 2.9314050674438477\n",
      "Epoch: 142 | Test Loss: 2.932968854904175 | Train Loss: 2.9282782077789307\n",
      "Epoch: 143 | Test Loss: 2.919989585876465 | Train Loss: 2.9199771881103516\n",
      "Epoch: 144 | Test Loss: 2.915844440460205 | Train Loss: 2.9173145294189453\n",
      "Epoch: 145 | Test Loss: 2.905153751373291 | Train Loss: 2.8989479541778564\n",
      "Epoch: 146 | Test Loss: 2.8830835819244385 | Train Loss: 2.9013845920562744\n",
      "Epoch: 147 | Test Loss: 2.878523111343384 | Train Loss: 2.879354238510132\n",
      "Epoch: 148 | Test Loss: 2.8651986122131348 | Train Loss: 2.866905450820923\n",
      "Epoch: 149 | Test Loss: 2.8495328426361084 | Train Loss: 2.8525516986846924\n",
      "Epoch: 150 | Test Loss: 2.8450429439544678 | Train Loss: 2.8451218605041504\n",
      "Epoch: 151 | Test Loss: 2.834273099899292 | Train Loss: 2.8335859775543213\n",
      "Epoch: 152 | Test Loss: 2.8203487396240234 | Train Loss: 2.8152573108673096\n",
      "Epoch: 153 | Test Loss: 2.8096582889556885 | Train Loss: 2.813458204269409\n",
      "Epoch: 154 | Test Loss: 2.7902190685272217 | Train Loss: 2.8069660663604736\n",
      "Epoch: 155 | Test Loss: 2.7865264415740967 | Train Loss: 2.7717833518981934\n",
      "Epoch: 156 | Test Loss: 2.7636189460754395 | Train Loss: 2.7756333351135254\n",
      "Epoch: 157 | Test Loss: 2.762301206588745 | Train Loss: 2.761133909225464\n",
      "Epoch: 158 | Test Loss: 2.7418620586395264 | Train Loss: 2.7562525272369385\n",
      "Epoch: 159 | Test Loss: 2.7332870960235596 | Train Loss: 2.73262095451355\n",
      "Epoch: 160 | Test Loss: 2.723583221435547 | Train Loss: 2.7217979431152344\n",
      "Epoch: 161 | Test Loss: 2.7114596366882324 | Train Loss: 2.715121269226074\n",
      "Epoch: 162 | Test Loss: 2.703706741333008 | Train Loss: 2.7048404216766357\n",
      "Epoch: 163 | Test Loss: 2.6951961517333984 | Train Loss: 2.705721616744995\n",
      "Epoch: 164 | Test Loss: 2.684807538986206 | Train Loss: 2.682509660720825\n",
      "Epoch: 165 | Test Loss: 2.6671273708343506 | Train Loss: 2.6821441650390625\n",
      "Epoch: 166 | Test Loss: 2.651838779449463 | Train Loss: 2.663560152053833\n",
      "Epoch: 167 | Test Loss: 2.6473395824432373 | Train Loss: 2.6566898822784424\n",
      "Epoch: 168 | Test Loss: 2.6346254348754883 | Train Loss: 2.637497901916504\n",
      "Epoch: 169 | Test Loss: 2.6263389587402344 | Train Loss: 2.6139602661132812\n",
      "Epoch: 170 | Test Loss: 2.6168203353881836 | Train Loss: 2.639315605163574\n",
      "Epoch: 171 | Test Loss: 2.5939712524414062 | Train Loss: 2.619798183441162\n",
      "Epoch: 172 | Test Loss: 2.5939528942108154 | Train Loss: 2.6121625900268555\n",
      "Epoch: 173 | Test Loss: 2.579073190689087 | Train Loss: 2.5979080200195312\n",
      "Epoch: 174 | Test Loss: 2.567580223083496 | Train Loss: 2.5780601501464844\n",
      "Epoch: 175 | Test Loss: 2.561314344406128 | Train Loss: 2.576605796813965\n",
      "Epoch: 176 | Test Loss: 2.551055669784546 | Train Loss: 2.5735743045806885\n",
      "Epoch: 177 | Test Loss: 2.5389509201049805 | Train Loss: 2.5589733123779297\n",
      "Epoch: 178 | Test Loss: 2.5295302867889404 | Train Loss: 2.547205686569214\n",
      "Epoch: 179 | Test Loss: 2.5245308876037598 | Train Loss: 2.5400025844573975\n",
      "Epoch: 180 | Test Loss: 2.515692710876465 | Train Loss: 2.5336508750915527\n",
      "Epoch: 181 | Test Loss: 2.50230073928833 | Train Loss: 2.524333953857422\n",
      "Epoch: 182 | Test Loss: 2.4837076663970947 | Train Loss: 2.5107531547546387\n",
      "Epoch: 183 | Test Loss: 2.4845287799835205 | Train Loss: 2.501312732696533\n",
      "Epoch: 184 | Test Loss: 2.4733853340148926 | Train Loss: 2.5054125785827637\n",
      "Epoch: 185 | Test Loss: 2.453822374343872 | Train Loss: 2.480224370956421\n",
      "Epoch: 186 | Test Loss: 2.457268714904785 | Train Loss: 2.470947265625\n",
      "Epoch: 187 | Test Loss: 2.437138795852661 | Train Loss: 2.4650094509124756\n",
      "Epoch: 188 | Test Loss: 2.4360389709472656 | Train Loss: 2.4542901515960693\n",
      "Epoch: 189 | Test Loss: 2.4272665977478027 | Train Loss: 2.450153350830078\n",
      "Epoch: 190 | Test Loss: 2.4150309562683105 | Train Loss: 2.443228006362915\n",
      "Epoch: 191 | Test Loss: 2.406170606613159 | Train Loss: 2.4557793140411377\n",
      "Epoch: 192 | Test Loss: 2.4014596939086914 | Train Loss: 2.4361236095428467\n",
      "Epoch: 193 | Test Loss: 2.3920750617980957 | Train Loss: 2.4509894847869873\n",
      "Epoch: 194 | Test Loss: 2.3887248039245605 | Train Loss: 2.4140894412994385\n",
      "Epoch: 195 | Test Loss: 2.3733043670654297 | Train Loss: 2.419062614440918\n",
      "Epoch: 196 | Test Loss: 2.3738298416137695 | Train Loss: 2.4261131286621094\n",
      "Epoch: 197 | Test Loss: 2.3684542179107666 | Train Loss: 2.401959180831909\n",
      "Epoch: 198 | Test Loss: 2.351745843887329 | Train Loss: 2.3904290199279785\n",
      "Epoch: 199 | Test Loss: 2.352024793624878 | Train Loss: 2.3947081565856934\n",
      "Epoch: 200 | Test Loss: 2.3421175479888916 | Train Loss: 2.377509117126465\n",
      "Epoch: 201 | Test Loss: 2.3351287841796875 | Train Loss: 2.386317491531372\n",
      "Epoch: 202 | Test Loss: 2.325923204421997 | Train Loss: 2.3522965908050537\n",
      "Epoch: 203 | Test Loss: 2.324906826019287 | Train Loss: 2.3496110439300537\n",
      "Epoch: 204 | Test Loss: 2.310816526412964 | Train Loss: 2.3745527267456055\n",
      "Epoch: 205 | Test Loss: 2.2985377311706543 | Train Loss: 2.3637611865997314\n",
      "Epoch: 206 | Test Loss: 2.2978925704956055 | Train Loss: 2.353344440460205\n",
      "Epoch: 207 | Test Loss: 2.295891046524048 | Train Loss: 2.333704948425293\n",
      "Epoch: 208 | Test Loss: 2.275653600692749 | Train Loss: 2.33475923538208\n",
      "Epoch: 209 | Test Loss: 2.2861294746398926 | Train Loss: 2.3433823585510254\n",
      "Epoch: 210 | Test Loss: 2.2795300483703613 | Train Loss: 2.3424346446990967\n",
      "Epoch: 211 | Test Loss: 2.275768518447876 | Train Loss: 2.3250160217285156\n",
      "Epoch: 212 | Test Loss: 2.2577991485595703 | Train Loss: 2.3056604862213135\n",
      "Epoch: 213 | Test Loss: 2.252251386642456 | Train Loss: 2.30696702003479\n",
      "Epoch: 214 | Test Loss: 2.253051519393921 | Train Loss: 2.3266537189483643\n",
      "Epoch: 215 | Test Loss: 2.2602739334106445 | Train Loss: 2.3167781829833984\n",
      "Epoch: 216 | Test Loss: 2.2420315742492676 | Train Loss: 2.311877965927124\n",
      "Epoch: 217 | Test Loss: 2.243828773498535 | Train Loss: 2.310286045074463\n",
      "Epoch: 218 | Test Loss: 2.2413671016693115 | Train Loss: 2.314192533493042\n",
      "Epoch: 219 | Test Loss: 2.235382556915283 | Train Loss: 2.3052306175231934\n",
      "Epoch: 220 | Test Loss: 2.232146978378296 | Train Loss: 2.296719789505005\n",
      "Epoch: 221 | Test Loss: 2.232475519180298 | Train Loss: 2.2820136547088623\n",
      "Epoch: 222 | Test Loss: 2.2158374786376953 | Train Loss: 2.2901132106781006\n",
      "Epoch: 223 | Test Loss: 2.2113265991210938 | Train Loss: 2.273993968963623\n",
      "Epoch: 224 | Test Loss: 2.208362579345703 | Train Loss: 2.2764663696289062\n",
      "Epoch: 225 | Test Loss: 2.20888090133667 | Train Loss: 2.2522289752960205\n",
      "Epoch: 226 | Test Loss: 2.200709342956543 | Train Loss: 2.2717621326446533\n",
      "Epoch: 227 | Test Loss: 2.200559616088867 | Train Loss: 2.255887269973755\n",
      "Epoch: 228 | Test Loss: 2.2054476737976074 | Train Loss: 2.2460672855377197\n",
      "Epoch: 229 | Test Loss: 2.19403076171875 | Train Loss: 2.2689499855041504\n",
      "Epoch: 230 | Test Loss: 2.200366973876953 | Train Loss: 2.2570595741271973\n",
      "Epoch: 231 | Test Loss: 2.1790401935577393 | Train Loss: 2.245203971862793\n",
      "Epoch: 232 | Test Loss: 2.175034284591675 | Train Loss: 2.2515804767608643\n",
      "Epoch: 233 | Test Loss: 2.1811814308166504 | Train Loss: 2.239971399307251\n",
      "Epoch: 234 | Test Loss: 2.1678109169006348 | Train Loss: 2.240934371948242\n",
      "Epoch: 235 | Test Loss: 2.1652767658233643 | Train Loss: 2.257274627685547\n",
      "Epoch: 236 | Test Loss: 2.167330265045166 | Train Loss: 2.2533304691314697\n",
      "Epoch: 237 | Test Loss: 2.1566622257232666 | Train Loss: 2.2321722507476807\n",
      "Epoch: 238 | Test Loss: 2.1610286235809326 | Train Loss: 2.2238101959228516\n",
      "Epoch: 239 | Test Loss: 2.1541426181793213 | Train Loss: 2.2186927795410156\n",
      "Epoch: 240 | Test Loss: 2.1475696563720703 | Train Loss: 2.2122933864593506\n",
      "Epoch: 241 | Test Loss: 2.1436710357666016 | Train Loss: 2.2156925201416016\n",
      "Epoch: 242 | Test Loss: 2.151888847351074 | Train Loss: 2.217855215072632\n",
      "Epoch: 243 | Test Loss: 2.1447298526763916 | Train Loss: 2.205703020095825\n",
      "Epoch: 244 | Test Loss: 2.1298763751983643 | Train Loss: 2.206970691680908\n",
      "Epoch: 245 | Test Loss: 2.138505697250366 | Train Loss: 2.1815757751464844\n",
      "Epoch: 246 | Test Loss: 2.13382625579834 | Train Loss: 2.2040958404541016\n",
      "Epoch: 247 | Test Loss: 2.1279711723327637 | Train Loss: 2.2141594886779785\n",
      "Epoch: 248 | Test Loss: 2.1212503910064697 | Train Loss: 2.197753667831421\n",
      "Epoch: 249 | Test Loss: 2.1289005279541016 | Train Loss: 2.2005767822265625\n",
      "Epoch: 250 | Test Loss: 2.113646984100342 | Train Loss: 2.1896793842315674\n",
      "Epoch: 251 | Test Loss: 2.107088327407837 | Train Loss: 2.192021131515503\n",
      "Epoch: 252 | Test Loss: 2.10402774810791 | Train Loss: 2.191307544708252\n",
      "Epoch: 253 | Test Loss: 2.1116344928741455 | Train Loss: 2.168755054473877\n",
      "Epoch: 254 | Test Loss: 2.1031816005706787 | Train Loss: 2.1941795349121094\n",
      "Epoch: 255 | Test Loss: 2.0941355228424072 | Train Loss: 2.1757924556732178\n",
      "Epoch: 256 | Test Loss: 2.0881245136260986 | Train Loss: 2.1643545627593994\n",
      "Epoch: 257 | Test Loss: 2.0947835445404053 | Train Loss: 2.180790662765503\n",
      "Epoch: 258 | Test Loss: 2.088186740875244 | Train Loss: 2.1555347442626953\n",
      "Epoch: 259 | Test Loss: 2.0872862339019775 | Train Loss: 2.165372610092163\n",
      "Epoch: 260 | Test Loss: 2.0901567935943604 | Train Loss: 2.1506242752075195\n",
      "Epoch: 261 | Test Loss: 2.078322410583496 | Train Loss: 2.157228946685791\n",
      "Epoch: 262 | Test Loss: 2.069892406463623 | Train Loss: 2.139265775680542\n",
      "Epoch: 263 | Test Loss: 2.068375587463379 | Train Loss: 2.162243604660034\n",
      "Epoch: 264 | Test Loss: 2.0715527534484863 | Train Loss: 2.167368173599243\n",
      "Epoch: 265 | Test Loss: 2.061349868774414 | Train Loss: 2.152614116668701\n",
      "Epoch: 266 | Test Loss: 2.062305450439453 | Train Loss: 2.152096748352051\n",
      "Epoch: 267 | Test Loss: 2.0541303157806396 | Train Loss: 2.1508190631866455\n",
      "Epoch: 268 | Test Loss: 2.061230421066284 | Train Loss: 2.1622235774993896\n",
      "Epoch: 269 | Test Loss: 2.05820894241333 | Train Loss: 2.1468935012817383\n",
      "Epoch: 270 | Test Loss: 2.0497841835021973 | Train Loss: 2.12249493598938\n",
      "Epoch: 271 | Test Loss: 2.0452404022216797 | Train Loss: 2.1309165954589844\n",
      "Epoch: 272 | Test Loss: 2.0480003356933594 | Train Loss: 2.133474111557007\n",
      "Epoch: 273 | Test Loss: 2.051119565963745 | Train Loss: 2.1164767742156982\n",
      "Epoch: 274 | Test Loss: 2.0404245853424072 | Train Loss: 2.128753900527954\n",
      "Epoch: 275 | Test Loss: 2.029493570327759 | Train Loss: 2.1066184043884277\n",
      "Epoch: 276 | Test Loss: 2.0292742252349854 | Train Loss: 2.103809118270874\n",
      "Epoch: 277 | Test Loss: 2.03163480758667 | Train Loss: 2.116284132003784\n",
      "Epoch: 278 | Test Loss: 2.0286190509796143 | Train Loss: 2.109558582305908\n",
      "Epoch: 279 | Test Loss: 2.028536796569824 | Train Loss: 2.1061787605285645\n",
      "Epoch: 280 | Test Loss: 2.0184106826782227 | Train Loss: 2.110668420791626\n",
      "Epoch: 281 | Test Loss: 2.0141894817352295 | Train Loss: 2.100600481033325\n",
      "Epoch: 282 | Test Loss: 2.0105245113372803 | Train Loss: 2.1092143058776855\n",
      "Epoch: 283 | Test Loss: 2.014202117919922 | Train Loss: 2.1082797050476074\n",
      "Epoch: 284 | Test Loss: 2.0184600353240967 | Train Loss: 2.0856409072875977\n",
      "Epoch: 285 | Test Loss: 2.0003373622894287 | Train Loss: 2.080164909362793\n",
      "Epoch: 286 | Test Loss: 1.997902750968933 | Train Loss: 2.088524341583252\n",
      "Epoch: 287 | Test Loss: 2.0003790855407715 | Train Loss: 2.066136360168457\n",
      "Epoch: 288 | Test Loss: 1.999068021774292 | Train Loss: 2.0766873359680176\n",
      "Epoch: 289 | Test Loss: 1.9864684343338013 | Train Loss: 2.0844829082489014\n",
      "Epoch: 290 | Test Loss: 1.9872770309448242 | Train Loss: 2.0799198150634766\n",
      "Epoch: 291 | Test Loss: 1.9829061031341553 | Train Loss: 2.096545934677124\n",
      "Epoch: 292 | Test Loss: 1.983482003211975 | Train Loss: 2.0818541049957275\n",
      "Epoch: 293 | Test Loss: 1.9804826974868774 | Train Loss: 2.0716874599456787\n",
      "Epoch: 294 | Test Loss: 1.9792288541793823 | Train Loss: 2.069973945617676\n",
      "Epoch: 295 | Test Loss: 1.972570776939392 | Train Loss: 2.0750536918640137\n",
      "Epoch: 296 | Test Loss: 1.9664608240127563 | Train Loss: 2.053962230682373\n",
      "Epoch: 297 | Test Loss: 1.9673163890838623 | Train Loss: 2.081894874572754\n",
      "Epoch: 298 | Test Loss: 1.9650530815124512 | Train Loss: 2.068110227584839\n",
      "Epoch: 299 | Test Loss: 1.9607477188110352 | Train Loss: 2.072547435760498\n",
      "Epoch: 300 | Test Loss: 1.9684399366378784 | Train Loss: 2.0321457386016846\n",
      "Epoch: 301 | Test Loss: 1.9573665857315063 | Train Loss: 2.045308828353882\n",
      "Epoch: 302 | Test Loss: 1.9582313299179077 | Train Loss: 2.0374398231506348\n",
      "Epoch: 303 | Test Loss: 1.9609172344207764 | Train Loss: 2.0511064529418945\n",
      "Epoch: 304 | Test Loss: 1.9516056776046753 | Train Loss: 2.0503592491149902\n",
      "Epoch: 305 | Test Loss: 1.9478532075881958 | Train Loss: 2.0261120796203613\n",
      "Epoch: 306 | Test Loss: 1.949069857597351 | Train Loss: 2.0313055515289307\n",
      "Epoch: 307 | Test Loss: 1.9503767490386963 | Train Loss: 2.0349464416503906\n",
      "Epoch: 308 | Test Loss: 1.9412497282028198 | Train Loss: 2.019730806350708\n",
      "Epoch: 309 | Test Loss: 1.9356430768966675 | Train Loss: 2.0175294876098633\n",
      "Epoch: 310 | Test Loss: 1.93611478805542 | Train Loss: 2.04101824760437\n",
      "Epoch: 311 | Test Loss: 1.9369088411331177 | Train Loss: 2.045687437057495\n",
      "Epoch: 312 | Test Loss: 1.9406241178512573 | Train Loss: 2.027656078338623\n",
      "Epoch: 313 | Test Loss: 1.9234726428985596 | Train Loss: 2.0048062801361084\n",
      "Epoch: 314 | Test Loss: 1.923789381980896 | Train Loss: 2.0347723960876465\n",
      "Epoch: 315 | Test Loss: 1.9259865283966064 | Train Loss: 2.011770486831665\n",
      "Epoch: 316 | Test Loss: 1.9162663221359253 | Train Loss: 2.0222320556640625\n",
      "Epoch: 317 | Test Loss: 1.9187746047973633 | Train Loss: 2.0127546787261963\n",
      "Epoch: 318 | Test Loss: 1.9178940057754517 | Train Loss: 2.009690046310425\n",
      "Epoch: 319 | Test Loss: 1.9102206230163574 | Train Loss: 1.9880521297454834\n",
      "Epoch: 320 | Test Loss: 1.912007212638855 | Train Loss: 1.9957091808319092\n",
      "Epoch: 321 | Test Loss: 1.9054303169250488 | Train Loss: 2.004517078399658\n",
      "Epoch: 322 | Test Loss: 1.9001123905181885 | Train Loss: 2.0041844844818115\n",
      "Epoch: 323 | Test Loss: 1.9084185361862183 | Train Loss: 1.9730912446975708\n",
      "Epoch: 324 | Test Loss: 1.8950835466384888 | Train Loss: 1.979712724685669\n",
      "Epoch: 325 | Test Loss: 1.8932770490646362 | Train Loss: 2.002659559249878\n",
      "Epoch: 326 | Test Loss: 1.8952720165252686 | Train Loss: 1.9881510734558105\n",
      "Epoch: 327 | Test Loss: 1.8957210779190063 | Train Loss: 1.9869918823242188\n",
      "Epoch: 328 | Test Loss: 1.8930529356002808 | Train Loss: 1.9890457391738892\n",
      "Epoch: 329 | Test Loss: 1.888877511024475 | Train Loss: 1.9686535596847534\n",
      "Epoch: 330 | Test Loss: 1.8818014860153198 | Train Loss: 1.9825462102890015\n",
      "Epoch: 331 | Test Loss: 1.8745317459106445 | Train Loss: 1.9752284288406372\n",
      "Epoch: 332 | Test Loss: 1.8708902597427368 | Train Loss: 1.9674409627914429\n",
      "Epoch: 333 | Test Loss: 1.8789337873458862 | Train Loss: 1.9633440971374512\n",
      "Epoch: 334 | Test Loss: 1.8757579326629639 | Train Loss: 1.9744713306427002\n",
      "Epoch: 335 | Test Loss: 1.8661764860153198 | Train Loss: 1.970872163772583\n",
      "Epoch: 336 | Test Loss: 1.8624906539916992 | Train Loss: 1.9611858129501343\n",
      "Epoch: 337 | Test Loss: 1.867690920829773 | Train Loss: 1.9542129039764404\n",
      "Epoch: 338 | Test Loss: 1.862464189529419 | Train Loss: 1.9517070055007935\n",
      "Epoch: 339 | Test Loss: 1.854770541191101 | Train Loss: 1.9533265829086304\n",
      "Epoch: 340 | Test Loss: 1.8615723848342896 | Train Loss: 1.943847417831421\n",
      "Epoch: 341 | Test Loss: 1.8568419218063354 | Train Loss: 1.9420760869979858\n",
      "Epoch: 342 | Test Loss: 1.8585405349731445 | Train Loss: 1.938643217086792\n",
      "Epoch: 343 | Test Loss: 1.8522354364395142 | Train Loss: 1.942062258720398\n",
      "Epoch: 344 | Test Loss: 1.8489803075790405 | Train Loss: 1.95071280002594\n",
      "Epoch: 345 | Test Loss: 1.8450580835342407 | Train Loss: 1.9411635398864746\n",
      "Epoch: 346 | Test Loss: 1.848654866218567 | Train Loss: 1.9402350187301636\n",
      "Epoch: 347 | Test Loss: 1.8408982753753662 | Train Loss: 1.931227445602417\n",
      "Epoch: 348 | Test Loss: 1.8387104272842407 | Train Loss: 1.9293960332870483\n",
      "Epoch: 349 | Test Loss: 1.8455978631973267 | Train Loss: 1.931448221206665\n",
      "Epoch: 350 | Test Loss: 1.83523428440094 | Train Loss: 1.9441173076629639\n",
      "Epoch: 351 | Test Loss: 1.830443263053894 | Train Loss: 1.9225915670394897\n",
      "Epoch: 352 | Test Loss: 1.8307353258132935 | Train Loss: 1.9244391918182373\n",
      "Epoch: 353 | Test Loss: 1.8235974311828613 | Train Loss: 1.9179219007492065\n",
      "Epoch: 354 | Test Loss: 1.8218727111816406 | Train Loss: 1.9240562915802002\n",
      "Epoch: 355 | Test Loss: 1.826087236404419 | Train Loss: 1.9134174585342407\n",
      "Epoch: 356 | Test Loss: 1.8182004690170288 | Train Loss: 1.9136812686920166\n",
      "Epoch: 357 | Test Loss: 1.8196461200714111 | Train Loss: 1.9276270866394043\n",
      "Epoch: 358 | Test Loss: 1.8067923784255981 | Train Loss: 1.909753680229187\n",
      "Epoch: 359 | Test Loss: 1.811705470085144 | Train Loss: 1.8970898389816284\n",
      "Epoch: 360 | Test Loss: 1.8112258911132812 | Train Loss: 1.899274230003357\n",
      "Epoch: 361 | Test Loss: 1.8062671422958374 | Train Loss: 1.9088488817214966\n",
      "Epoch: 362 | Test Loss: 1.8172883987426758 | Train Loss: 1.9094353914260864\n",
      "Epoch: 363 | Test Loss: 1.8017797470092773 | Train Loss: 1.8935517072677612\n",
      "Epoch: 364 | Test Loss: 1.8081642389297485 | Train Loss: 1.898202896118164\n",
      "Epoch: 365 | Test Loss: 1.798858880996704 | Train Loss: 1.8833428621292114\n",
      "Epoch: 366 | Test Loss: 1.8002371788024902 | Train Loss: 1.8940494060516357\n",
      "Epoch: 367 | Test Loss: 1.7956814765930176 | Train Loss: 1.8808860778808594\n",
      "Epoch: 368 | Test Loss: 1.7939918041229248 | Train Loss: 1.9039112329483032\n",
      "Epoch: 369 | Test Loss: 1.8011722564697266 | Train Loss: 1.8891018629074097\n",
      "Epoch: 370 | Test Loss: 1.7882176637649536 | Train Loss: 1.8950324058532715\n",
      "Epoch: 371 | Test Loss: 1.785992980003357 | Train Loss: 1.885648488998413\n",
      "Epoch: 372 | Test Loss: 1.7765907049179077 | Train Loss: 1.8775469064712524\n",
      "Epoch: 373 | Test Loss: 1.7790364027023315 | Train Loss: 1.8837651014328003\n",
      "Epoch: 374 | Test Loss: 1.7801809310913086 | Train Loss: 1.8784085512161255\n",
      "Epoch: 375 | Test Loss: 1.7820370197296143 | Train Loss: 1.8565422296524048\n",
      "Epoch: 376 | Test Loss: 1.7800968885421753 | Train Loss: 1.8678770065307617\n",
      "Epoch: 377 | Test Loss: 1.7746044397354126 | Train Loss: 1.872829556465149\n",
      "Epoch: 378 | Test Loss: 1.768822193145752 | Train Loss: 1.8705272674560547\n",
      "Epoch: 379 | Test Loss: 1.7683309316635132 | Train Loss: 1.8724932670593262\n",
      "Epoch: 380 | Test Loss: 1.7593199014663696 | Train Loss: 1.8663564920425415\n",
      "Epoch: 381 | Test Loss: 1.7624722719192505 | Train Loss: 1.8652724027633667\n",
      "Epoch: 382 | Test Loss: 1.7602347135543823 | Train Loss: 1.8625051975250244\n",
      "Epoch: 383 | Test Loss: 1.7600736618041992 | Train Loss: 1.8586435317993164\n",
      "Epoch: 384 | Test Loss: 1.7576395273208618 | Train Loss: 1.8505274057388306\n",
      "Epoch: 385 | Test Loss: 1.753209114074707 | Train Loss: 1.8504265546798706\n",
      "Epoch: 386 | Test Loss: 1.7583014965057373 | Train Loss: 1.8608752489089966\n",
      "Epoch: 387 | Test Loss: 1.7493358850479126 | Train Loss: 1.8674448728561401\n",
      "Epoch: 388 | Test Loss: 1.7512712478637695 | Train Loss: 1.8434693813323975\n",
      "Epoch: 389 | Test Loss: 1.7424488067626953 | Train Loss: 1.8454596996307373\n",
      "Epoch: 390 | Test Loss: 1.7444325685501099 | Train Loss: 1.8611524105072021\n",
      "Epoch: 391 | Test Loss: 1.7436554431915283 | Train Loss: 1.8638005256652832\n",
      "Epoch: 392 | Test Loss: 1.7391525506973267 | Train Loss: 1.847486138343811\n",
      "Epoch: 393 | Test Loss: 1.7426866292953491 | Train Loss: 1.8405718803405762\n",
      "Epoch: 394 | Test Loss: 1.7401587963104248 | Train Loss: 1.8417326211929321\n",
      "Epoch: 395 | Test Loss: 1.7362300157546997 | Train Loss: 1.832812786102295\n",
      "Epoch: 396 | Test Loss: 1.7390062808990479 | Train Loss: 1.8338762521743774\n",
      "Epoch: 397 | Test Loss: 1.7304447889328003 | Train Loss: 1.826794981956482\n",
      "Epoch: 398 | Test Loss: 1.7251514196395874 | Train Loss: 1.8191250562667847\n",
      "Epoch: 399 | Test Loss: 1.72439444065094 | Train Loss: 1.822053074836731\n",
      "Epoch: 400 | Test Loss: 1.7177550792694092 | Train Loss: 1.8413515090942383\n",
      "Epoch: 401 | Test Loss: 1.721289038658142 | Train Loss: 1.8421801328659058\n",
      "Epoch: 402 | Test Loss: 1.7125061750411987 | Train Loss: 1.848059892654419\n",
      "Epoch: 403 | Test Loss: 1.7125070095062256 | Train Loss: 1.8056118488311768\n",
      "Epoch: 404 | Test Loss: 1.7211599349975586 | Train Loss: 1.814034342765808\n",
      "Epoch: 405 | Test Loss: 1.7133288383483887 | Train Loss: 1.8287278413772583\n",
      "Epoch: 406 | Test Loss: 1.7088580131530762 | Train Loss: 1.8248801231384277\n",
      "Epoch: 407 | Test Loss: 1.7210755348205566 | Train Loss: 1.8239752054214478\n",
      "Epoch: 408 | Test Loss: 1.707899570465088 | Train Loss: 1.8138141632080078\n",
      "Epoch: 409 | Test Loss: 1.7110432386398315 | Train Loss: 1.8102543354034424\n",
      "Epoch: 410 | Test Loss: 1.7049695253372192 | Train Loss: 1.8041919469833374\n",
      "Epoch: 411 | Test Loss: 1.7051141262054443 | Train Loss: 1.8156516551971436\n",
      "Epoch: 412 | Test Loss: 1.7017918825149536 | Train Loss: 1.8138006925582886\n",
      "Epoch: 413 | Test Loss: 1.704806923866272 | Train Loss: 1.8152422904968262\n",
      "Epoch: 414 | Test Loss: 1.7003650665283203 | Train Loss: 1.8093223571777344\n",
      "Epoch: 415 | Test Loss: 1.6989580392837524 | Train Loss: 1.8114259243011475\n",
      "Epoch: 416 | Test Loss: 1.690904974937439 | Train Loss: 1.8102926015853882\n",
      "Epoch: 417 | Test Loss: 1.6958436965942383 | Train Loss: 1.7962753772735596\n",
      "Epoch: 418 | Test Loss: 1.6841384172439575 | Train Loss: 1.8085108995437622\n",
      "Epoch: 419 | Test Loss: 1.6866543292999268 | Train Loss: 1.7965198755264282\n",
      "Epoch: 420 | Test Loss: 1.6848533153533936 | Train Loss: 1.7879934310913086\n",
      "Epoch: 421 | Test Loss: 1.68272864818573 | Train Loss: 1.7970860004425049\n",
      "Epoch: 422 | Test Loss: 1.682523488998413 | Train Loss: 1.8088595867156982\n",
      "Epoch: 423 | Test Loss: 1.6817775964736938 | Train Loss: 1.794845461845398\n",
      "Epoch: 424 | Test Loss: 1.6790851354599 | Train Loss: 1.8005439043045044\n",
      "Epoch: 425 | Test Loss: 1.675586462020874 | Train Loss: 1.7850919961929321\n",
      "Epoch: 426 | Test Loss: 1.6762959957122803 | Train Loss: 1.788366675376892\n",
      "Epoch: 427 | Test Loss: 1.6759893894195557 | Train Loss: 1.7859078645706177\n",
      "Epoch: 428 | Test Loss: 1.6740657091140747 | Train Loss: 1.7826160192489624\n",
      "Epoch: 429 | Test Loss: 1.6853010654449463 | Train Loss: 1.7999597787857056\n",
      "Epoch: 430 | Test Loss: 1.6690945625305176 | Train Loss: 1.7785615921020508\n",
      "Epoch: 431 | Test Loss: 1.6718244552612305 | Train Loss: 1.7705689668655396\n",
      "Epoch: 432 | Test Loss: 1.670595407485962 | Train Loss: 1.7677536010742188\n",
      "Epoch: 433 | Test Loss: 1.6647590398788452 | Train Loss: 1.7586528062820435\n",
      "Epoch: 434 | Test Loss: 1.665938138961792 | Train Loss: 1.7769337892532349\n",
      "Epoch: 435 | Test Loss: 1.6650631427764893 | Train Loss: 1.7814407348632812\n",
      "Epoch: 436 | Test Loss: 1.6591966152191162 | Train Loss: 1.7586743831634521\n",
      "Epoch: 437 | Test Loss: 1.6527594327926636 | Train Loss: 1.7567028999328613\n",
      "Epoch: 438 | Test Loss: 1.6540480852127075 | Train Loss: 1.7814456224441528\n",
      "Epoch: 439 | Test Loss: 1.6548675298690796 | Train Loss: 1.7761458158493042\n",
      "Epoch: 440 | Test Loss: 1.6441096067428589 | Train Loss: 1.7677351236343384\n",
      "Epoch: 441 | Test Loss: 1.6468826532363892 | Train Loss: 1.75527024269104\n",
      "Epoch: 442 | Test Loss: 1.64349365234375 | Train Loss: 1.757920742034912\n",
      "Epoch: 443 | Test Loss: 1.6395263671875 | Train Loss: 1.7493184804916382\n",
      "Epoch: 444 | Test Loss: 1.647279977798462 | Train Loss: 1.7672734260559082\n",
      "Epoch: 445 | Test Loss: 1.6362918615341187 | Train Loss: 1.7665992975234985\n",
      "Epoch: 446 | Test Loss: 1.6393061876296997 | Train Loss: 1.7658368349075317\n",
      "Epoch: 447 | Test Loss: 1.6459344625473022 | Train Loss: 1.7570302486419678\n",
      "Epoch: 448 | Test Loss: 1.6391899585723877 | Train Loss: 1.73958158493042\n",
      "Epoch: 449 | Test Loss: 1.6383171081542969 | Train Loss: 1.773300051689148\n"
     ]
    }
   ],
   "source": [
    "size = window\n",
    "epochs = 450\n",
    "model = model.to(\"cuda\")\n",
    "training_data = training_data.to(\"cuda\").float()\n",
    "train_ys = train_ys.to(device=\"cuda\").float()\n",
    "test_ys = test_ys.to(device=\"cuda\").float()\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    prediction = model(training_data)[:, -1]\n",
    "    loss = loss_fn(prediction, train_ys)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(testing_data)[:, -1]\n",
    "        test_loss = loss_fn(prediction, test_ys)\n",
    "\n",
    "    train_losses.append(loss)\n",
    "    test_losses.append(test_loss)\n",
    "    print(f\"Epoch: {epoch} | Test Loss: {loss} | Train Loss: {test_loss}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def numpy(v):\n",
    "    return v.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "plot_losses = [list(map(lambda v: numpy(v), l)) for l in [train_losses, test_losses]]\n",
    "plot_losses[0][0]\n",
    "plt.plot(plot_losses[0])\n",
    "plt.plot(plot_losses[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineWalk(image, model, coordinates, iters, history=None):\n",
    "    if history == None:\n",
    "        history = [coordinates[0]]\n",
    "        # history = []\n",
    "    # input for this iter\n",
    "    patches = extract_patches(image, coordinates, window).unsqueeze(0)\n",
    "\n",
    "    model_guess = model(patches)[0, -1]\n",
    "    tt(model_guess)\n",
    "    if (model_guess.round().int() == torch.tensor([0, 0]).to(\"cuda\")).all():\n",
    "        print(\"Next model jump is (0,0). Done!\")\n",
    "        return history\n",
    "    new_coordinates = torch.stack([coordinates[-1], coordinates[-1]])\n",
    "    new_coordinates[-1] += model_guess\n",
    "\n",
    "    history.append(new_coordinates[-1])\n",
    "    if iters >= 1500:\n",
    "        return history\n",
    "    return lineWalk(image, model, new_coordinates, iters + 1, history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_coord = get_line(1)[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor (dtype: torch.float64)\n",
      "    |  (device: cpu)\n",
      "    |__dim_0 (2)\n",
      "    |__dim_1 (2)\n"
     ]
    }
   ],
   "source": [
    "tt(starting_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "extract_patches() missing 1 required positional argument: 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m walk \u001b[39m=\u001b[39m lineWalk(image, model, starting_coord\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m), \u001b[39m0\u001b[39;49m, history\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m      2\u001b[0m tt(walk)\n\u001b[1;32m      3\u001b[0m walk \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(walk)\n",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m, in \u001b[0;36mlineWalk\u001b[0;34m(image, model, coordinates, iters, history)\u001b[0m\n\u001b[1;32m      3\u001b[0m     history \u001b[39m=\u001b[39m [coordinates[\u001b[39m0\u001b[39m]]\n\u001b[1;32m      4\u001b[0m     \u001b[39m# history = []\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# input for this iter\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m patches \u001b[39m=\u001b[39m extract_patches(image, coordinates, window)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m model_guess \u001b[39m=\u001b[39m model(patches)[\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m      9\u001b[0m tt(model_guess)\n",
      "\u001b[0;31mTypeError\u001b[0m: extract_patches() missing 1 required positional argument: 'size'"
     ]
    }
   ],
   "source": [
    "walk = lineWalk(image, model, starting_coord.to(\"cuda\"), 0, history=None)\n",
    "tt(walk)\n",
    "walk = torch.stack(walk)\n",
    "tt(walk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor (dtype: torch.float64)\n",
      "    |__dim_0 (1502)\n",
      "    |__dim_1 (3)\n"
     ]
    }
   ],
   "source": [
    "csvable_points = torch.cat([torch.ones(walk.shape[0], 1).to(\"cuda\") * 0, walk], dim=1)\n",
    "tt(csvable_points)\n",
    "df = pd.DataFrame(numpy(csvable_points), columns=[\"index\", \"x\", \"y\"])\n",
    "\n",
    "df.to_csv(\"./model_walk.csv\", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
